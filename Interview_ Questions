I. GENERAL AI & ML CONCEPTS
1. What is the difference between AI, Machine Learning, and Deep Learning?
AI (Artificial Intelligence): A broad field that aims to create systems that can mimic human intelligence.
Machine Learning (ML): A subset of AI that enables machines to learn patterns from data and make decisions.
Deep Learning (DL): A specialized subset of ML using neural networks with many layers (deep networks) for tasks like image and speech recognition.

2. What are the main types of Machine Learning?
Supervised Learning: Trains on labeled data (e.g., classification, regression).
Unsupervised Learning: Finds patterns in unlabeled data (e.g., clustering).
Reinforcement Learning: Learns optimal actions through rewards and penalties.

3. Explain the difference between supervised and unsupervised learning.
Supervised: The model learns from input-output pairs (e.g., predicting genre from audio features).
Unsupervised: The model tries to identify hidden patterns without any labels (e.g., clustering songs based on features).

4. What is overfitting and underfitting?
Overfitting: The model learns noise and performs well on training data but poorly on new data.
Underfitting: The model is too simple and cannot capture the underlying patterns.

5. How do you evaluate a machine learning model?
Using metrics like accuracy, precision, recall, F1-score, confusion matrix, and ROC-AUC, depending on the problem type.

6. What is the bias-variance trade-off?
It describes the balance between bias (error from wrong assumptions) and variance (error from sensitivity to small fluctuations in the training set). Ideal models minimize both.

7. Explain cross-validation and why it's important.
It splits the dataset into multiple train-test splits (like K-fold) to ensure the model performs well across different subsets and prevents overfitting.

8. What are precision, recall, F1-score, and accuracy?
Accuracy: Correct predictions / total predictions.
Precision: True positives / predicted positives.
Recall: True positives / actual positives.
F1-score: Harmonic mean of precision and recall.

9. What is the difference between classification and regression?
Classification: Predicts categories (e.g., genre).
Regression: Predicts continuous values (e.g., song duration).

10. What are some real-world applications of AI/ML?
Spam detection, voice assistants, recommendation systems (e.g., Spotify), medical diagnosis, fraud detection, autonomous vehicles.

II. DATA PREPROCESSING & FEATURE ENGINEERING
1. How do you handle missing data in a dataset?
Remove rows/columns with too many missing values.
Impute missing values using:
Mean/median/mode for numerical data.
Most frequent or constant value for categorical data.
Advanced methods like KNN imputation or regression imputation.

2. What is normalization and standardization?
Normalization (Min-Max Scaling): Scales values between 0 and 1.
X norm =𝑋 −𝑋min/Xmax-Xmin

​Standardization (Z-score scaling): Centers data around 0 with unit variance.
𝑋std=𝑋−𝜇/σ
Use standardization for algorithms like SVM or KNN, and normalization when the scale needs to be between 0 and 1.

3. What is one-hot encoding? When would you use it?
Converts categorical variables into binary vectors.
Example: Color = [Red, Green, Blue] → [1, 0, 0], [0, 1, 0], [0, 0, 1].
Use when categories are nominal (no inherent order).

4. How do you handle categorical variables?
One-Hot Encoding: For nominal data.
Label Encoding: Converts categories to numeric labels (useful for tree-based models).
Target Encoding / Frequency Encoding: More advanced methods when high cardinality is involved.

5. What is feature selection and why is it important?
Selecting the most relevant features improves model performance and reduces overfitting.
Techniques:
Filter methods: Correlation, chi-squared test.
Wrapper methods: Recursive Feature Elimination (RFE).
Embedded methods: Lasso regularization.

6. What is dimensionality reduction? Explain PCA.
Reducing the number of input variables to speed up training and avoid the curse of dimensionality.
PCA (Principal Component Analysis):
Transforms features into a new coordinate system.
Maximizes variance in fewer dimensions.

7. What is the curse of dimensionality?
As features increase, data becomes sparse, making pattern recognition and model training difficult.
It affects distance-based models (e.g., KNN, clustering).

8. What is feature scaling and when should it be applied?
Feature scaling standardizes the range of independent variables.
Apply before using:
Distance-based models (KNN, SVM).
Gradient-based algorithms (logistic regression, neural nets).

9. How do you deal with imbalanced datasets?
Resampling: Oversample the minority class or undersample the majority class.
SMOTE (Synthetic Minority Over-sampling Technique).
Use appropriate evaluation metrics like F1-score or AUC-ROC.
Use class weights in model training.

10. Explain the role of EDA (Exploratory Data Analysis) in ML.
Helps in understanding the dataset via visualization and statistics.
Key goals:
Discover patterns, trends, anomalies.
Identify data distribution.
Choose appropriate preprocessing strategies.
Guide feature selection.

III. MACHINE LEARNING ALGORITHMS
1. How does the Decision Tree algorithm work?
A tree-like model of decisions:
Splits data on feature values using metrics like Gini impurity or entropy (information gain).
Continues splitting until a stopping criterion is met (e.g., depth, purity).
Easy to interpret but prone to overfitting.

2. What is the difference between bagging and boosting?
Bagging (Bootstrap Aggregating):
Builds multiple independent models on bootstrapped datasets.
Final prediction is made by majority vote (classification) or average (regression).
Example: Random Forest.
Boosting:
Builds models sequentially, each learning from the errors of the previous.
Focuses on difficult cases.
Example: AdaBoost, Gradient Boosting, XGBoost.

3. Explain how the K-Nearest Neighbors algorithm works.
Non-parametric, instance-based learning.
For a new point, it finds the 'k' closest training samples using distance (e.g., Euclidean).
Predicts the majority class (classification) or average value (regression).
Sensitive to feature scaling.

4. What is the intuition behind Support Vector Machines?
SVM finds the optimal hyperplane that best separates data into classes.
Maximizes the margin between support vectors (closest points from each class).
Can use kernels to handle non-linearly separable data.

5. How does Naive Bayes classifier work?
Based on Bayes’ Theorem:

𝑃
(
𝐴
∣
𝐵
)
=
𝑃
(
𝐵
∣
𝐴
)
⋅
𝑃
(
𝐴
)
𝑃
(
𝐵
)
P(A∣B)= 
P(B)
P(B∣A)⋅P(A)
​
 Assumes independence between features.
Works well with high-dimensional text data (e.g., spam detection).

6. What is the difference between Random Forest and XGBoost?
Random Forest:
Ensemble of decision trees (bagging).
Reduces variance.
Simple and robust.
XGBoost (Extreme Gradient Boosting):
Ensemble of trees built sequentially (boosting).
More accurate and faster due to regularization and efficient implementation.
Often wins Kaggle competitions.

7. How do gradient descent and stochastic gradient descent differ?
Gradient Descent:
Computes gradient over entire dataset.
Slower but stable.
Stochastic Gradient Descent (SGD):
Computes gradient using a single random sample.
Faster but noisier.
Mini-batch Gradient Descent:
Compromise: Uses a small batch of samples.

8. Explain logistic regression and where it’s used.
A classification algorithm based on the logistic (sigmoid) function.

𝜎
(
𝑧
)
=
1
1
+
𝑒
−
𝑧
σ(z)= 
1+e 
−z
 
1
​
Outputs probabilities; thresholded for binary classification.
Used in spam detection, medical diagnosis, etc.

9. What are ensemble models?
Combine predictions from multiple models to improve accuracy and robustness.
Types:
Bagging (e.g., Random Forest)
Boosting (e.g., XGBoost, AdaBoost)
Stacking (meta-model learns from base model outputs)

10. What is the difference between L1 and L2 regularization?
Both prevent overfitting by adding a penalty term:
L1 (Lasso): Adds absolute value of coefficients → can reduce some coefficients to zero (feature selection).

Loss
+
𝜆
∑
∣
𝑤
∣
Loss+λ∑∣w∣
L2 (Ridge): Adds squared value of coefficients → shrinks coefficients but doesn't make them zero.

Loss
+
𝜆
∑
𝑤
2
Loss+λ∑w 
2
 
IV. DEEP LEARNING & NEURAL NETWORKS
1. What is a perceptron?
The basic unit of a neural network.
Takes weighted inputs, applies an activation function, and produces an output.
For binary classification:
𝑦
=
activation
(
𝑤
1
𝑥
1
+
𝑤
2
𝑥
2
+
.
.
.
+
𝑤
𝑛
𝑥
𝑛
+
𝑏
)
y=activation(w 
1
​
 x 
1
​
 +w 
2
​
 x 
2
​
 +...+w 
n
​
 x 
n
​
 +b)
2. How do activation functions like ReLU, Sigmoid, and Tanh work?

ReLU (Rectified Linear Unit):

𝑓
(
𝑥
)
=
max
⁡
(
0
,
𝑥
)
f(x)=max(0,x)
Fast, widely used in hidden layers.

Sigmoid:

𝑓
(
𝑥
)
=
1
1
+
𝑒
−
𝑥
f(x)= 
1+e 
−x
 
1
​
 
Outputs values between 0 and 1; used for binary classification.

Tanh:

𝑓
(
𝑥
)
=
𝑒
𝑥
−
𝑒
−
𝑥
𝑒
𝑥
+
𝑒
−
𝑥
f(x)= 
e 
x
 +e 
−x
 
e 
x
 −e 
−x
 
​
 
Outputs between -1 and 1; better centered than sigmoid.

3. What are epochs, batch size, and learning rate?
Epoch: One full pass over the entire training dataset.
Batch Size: Number of samples processed before model weights are updated.
Learning Rate: Step size at each iteration during optimization.

4. What is the vanishing gradient problem?
In deep networks, gradients become very small in earlier layers during backpropagation.
This slows or stops learning.
Common in sigmoid/tanh activations.
Solution: Use ReLU, batch normalization, or better initialization.

5. What is the difference between CNN and RNN?
CNN (Convolutional Neural Network):
Best for images. Uses filters to detect spatial patterns.
RNN (Recurrent Neural Network):
Best for sequential data (e.g., text, time series). Maintains memory of previous inputs.

6. How does an LSTM work and where is it used?
LSTM (Long Short-Term Memory):
Special RNN that remembers long-term dependencies.
Contains input, output, and forget gates.
Used in: Text generation, speech recognition, time-series forecasting.

7. What are convolutional layers in CNN?
Apply filters to input data to extract features like edges, textures, etc.
Reduces dimensionality while preserving spatial relationships.
Commonly followed by pooling layers (e.g., Max Pooling).

8. What is transfer learning?
Use a pre-trained model (e.g., on ImageNet) and fine-tune it on your dataset.
Speeds up training and improves performance, especially with limited data.

9. What is dropout and why is it used?
A regularization technique.
Randomly "drops" a fraction of neurons during training.
Prevents overfitting by ensuring the model doesn’t rely on specific neurons.

10. How do you prevent overfitting in deep learning models?
Techniques include:
Dropout
Early stopping
L1/L2 regularization
Data augmentation
Using more data

V. NATURAL LANGUAGE PROCESSING
1. What is tokenization in NLP?
The process of splitting text into smaller units, like words or subwords (tokens).
Example: "I love AI" → ["I", "love", "AI"]
Helps machines understand and process human language.

2. How do word embeddings like Word2Vec or GloVe work?
They convert words into dense vectors based on their meaning and context.
Word2Vec: Learns word associations using a neural network.
GloVe: Uses global word co-occurrence statistics.
Similar words have similar vectors, enabling semantic understanding.

3. What is the difference between stemming and lemmatization?
Stemming: Cuts words to their base/root form by chopping suffixes.
"running" → "run", "flies" → "fli"
May not produce actual words.
Lemmatization: Converts words to their meaningful root form using grammar rules.
"running" → "run", "better" → "good"
More accurate and linguistically valid.

4. What is TF-IDF and why is it used?
Term Frequency–Inverse Document Frequency: A method to weigh words in documents.
TF: How often a word appears in a document.
IDF: How unique a word is across all documents.
Helps in feature extraction by highlighting important, rare terms and downweighting common ones like "the", "is".

5. What are Transformers in NLP?
A deep learning architecture that uses self-attention mechanisms.
Can handle long-range dependencies better than RNNs.
Transformers are the backbone of powerful models like BERT, GPT, and T5.
Useful for translation, summarization, Q&A, and more.

VI. MODEL DEPLOYMENT & MLOps
1. How do you save and load a trained model in Python?
For scikit-learn models:
python
Copy
Edit
import joblib
joblib.dump(model, 'model.pkl')         # Save
model = joblib.load('model.pkl')        # Load
For Keras models:

python
Copy
Edit
model.save('model.h5')                  # Save
from tensorflow.keras.models import load_model
model = load_model('model.h5')          # Load

2. What is model drift and how do you monitor it?
Model drift occurs when a model's performance degrades over time due to changes in data patterns.
Types:
Concept drift: The relationship between input and output changes.
Data drift: The input data distribution changes.
Monitoring: Use statistical tests, dashboards (e.g., EvidentlyAI), or retrain periodically.

3. How do you deploy a machine learning model as an API?
Use Flask or FastAPI to wrap the model logic inside an endpoint.
Example using Flask:

python
Copy
Edit
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)
model = joblib.load('model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    prediction = model.predict([data['features']])
    return jsonify({'prediction': prediction.tolist()})

app.run()

4. What are common tools for deploying ML models?
Flask, FastAPI – lightweight web frameworks to serve models.
Docker – to containerize the model and dependencies for easy deployment.
Streamlit – for building interactive ML apps with minimal code.
Heroku, AWS, GCP, Azure – cloud platforms for hosting models/APIs.

5. Explain the CI/CD pipeline in MLOps.
CI (Continuous Integration): Automatically test and validate model code when it's committed.
CD (Continuous Deployment/Delivery): Automate the deployment of models to production.
Pipeline example:
Code → Test → Train model → Validate → Package → Deploy → Monitor
Tools: GitHub Actions, Jenkins, MLflow, DVC, Kubeflow, Airflow.





